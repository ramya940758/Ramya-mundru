{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramya940758/Ramya-mundru/blob/main/mundru_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Collect all the customer reviews of a product (you can choose any porduct) on amazon."
      ],
      "metadata": {
        "id": "ymm6H6NXoLNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OX3s02ebCEG",
        "outputId": "c6c6ff88-d5cf-44f4-f46c-f7838d4dc593"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Define the URL of the IMDb page for \"Baahubali\"\n",
        "url = \"https://www.imdb.com/title/tt2631186/reviews?ref_=tt_ql_3\"\n",
        "\n",
        "# Send an HTTP GET request to the IMDb page\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Find and extract user reviews\n",
        "    review_elements = soup.find_all(\"div\", class_=\"text show-more__control\")\n",
        "    user_reviews = [review.get_text() for review in review_elements]\n",
        "\n",
        "    # Prepare data for saving to a CSV file\n",
        "    data = [{\"user_review\": review} for review in user_reviews]\n",
        "\n",
        "    # Define the CSV file name\n",
        "    csv_file = \"baahubali_reviews.csv\"\n",
        "\n",
        "    # Write the data to a CSV file\n",
        "    with open(csv_file, \"w\", newline=\"\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"user_review\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"Data saved to {csv_file}\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve data from IMDb. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgWdCz2DbBsQ",
        "outputId": "686ae940-79e2-4bba-e4b9-97bcf88a01f5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to baahubali_reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJhGD7rt0eMX",
        "outputId": "8dcd011e-be70-49f1-81e5-472a385971be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB."
      ],
      "metadata": {
        "id": "-F4mZHGPoaM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Function to scrape user reviews from IMDB for a given movie\n",
        "def scrape_terrifier_reviews(movie_id, num_reviews):\n",
        "    base_url = f\"https://www.imdb.com/title/{movie_id}/reviews\"\n",
        "    reviews = []\n",
        "\n",
        "    for start in range(1, num_reviews + 1, 25):\n",
        "        url = f\"{base_url}?sort=submissionDate&dir=desc&ratingFilter=0&start={start}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve reviews from {url}\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_divs = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "        for review_div in review_divs:\n",
        "            text = review_div.get_text(strip=True)\n",
        "            reviews.append(text)\n",
        "\n",
        "    return reviews\n",
        "\n",
        "# Example: Scrape reviews for a terrifier movie\n",
        "movie_id = \"tt10403420\"\n",
        "num_reviews = 10000\n",
        "\n",
        "reviews = scrape_terrifier_reviews(movie_id, num_reviews)\n",
        "\n",
        "# Save reviews to a CSV file\n",
        "df = pd.DataFrame({'Reviews': reviews})\n",
        "df.to_csv('reviews_terrifier.csv', index=False)\n",
        "\n",
        "print(f\"Scraped {len(reviews)} reviews and saved to 'reviews_terrifier.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO3KorXj8DUF",
        "outputId": "b213d1ea-165a-4cc0-c7f2-df717ac6f45c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 10000 reviews and saved to 'reviews_terrifier.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk textblob\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73JoeMCXRqky",
        "outputId": "a860e0d1-8657-458f-c09b-db865ed187f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the NLTK stopwords and WordNet lemmatizer data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('baahubali_reviews.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpvPR-9gdPD6",
        "outputId": "6c03cc7b-d270-4f34-e3f0-901aa015fb9b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-zdII7pdQjK",
        "outputId": "bc8a5147-c091-44a3-9039-363b974185eb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['user_review'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the NLTK stopwords and WordNet lemmatizer data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('baahubali_reviews.csv')\n",
        "\n",
        "# Function to clean and preprocess text\n",
        "def clean_text(text):\n",
        "    # Remove special characters and punctuations\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Initialize the stemmer and lemmatizer\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Stem and lemmatize the words\n",
        "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words]\n",
        "\n",
        "    # Join the cleaned words back into a single string\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "#Apply the cleaning function to your data and save it in a new column\n",
        "df['user_review'] = df['user_review'].apply(clean_text)\n",
        "\n",
        "# Save the DataFrame with cleaned text to a new CSV file\n",
        "df.to_csv('clean_data.csv', index=False)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCqwHA9mNI-a",
        "outputId": "3acd534f-36f3-4b6b-f4c2-81926fc3f225"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          user_review\n",
            "0   eagerli wait film disappointedbaahubali one vi...\n",
            "1   person bit movi sure person heard baahubali co...\n",
            "2   year sheer hardwork mani peopl work heart visu...\n",
            "3   im big fan rajamouli bias review rate gave raj...\n",
            "4   test whether movi clich pathet romanc hero woo...\n",
            "5   great cinematographi concept stori nice direct...\n",
            "6   cant imagin im one found way twopart indian ep...\n",
            "7   watch recent air tv noth superhero movi main p...\n",
            "8   would simpli say get away anyon give review go...\n",
            "9   gener like prabha movi rajamouli movi eega rea...\n",
            "10  movi cannot even consid b movi person like mus...\n",
            "11  usual dont watch bollywood masala movi mostli ...\n",
            "12  hallmark indian cinema hang head shamei get be...\n",
            "13  blockbust climax absolut interv movi man turn ...\n",
            "14  portray biggest indian spectacl till date heav...\n",
            "15  dont fool imdb rate movi im south indian didnt...\n",
            "16  announc tollywood embark first kind visual epi...\n",
            "17  genuin worst film ive ever seenit pace way fas...\n",
            "18  sometim movi come along caus revolut film hist...\n",
            "19  aw movi made aw hype could accept local region...\n",
            "20  word epic doesnt fulli describ movi lot flaw o...\n",
            "21  got noth new thousand time use outdat stori li...\n",
            "22  overr movi deserv give other rate movi fun wat...\n",
            "23  floor technic excel visual effect stand ovat c...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvgvpBWdiV0M",
        "outputId": "7cdfc415-ec8c-45ad-b3ca-8855a8103a7f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPACKDHbilwR",
        "outputId": "ed5f2179-b169-4d80-849f-0dac1ecb1cb8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8c2a12-9041-462e-c20b-4c9f513cab7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Sentence:\n",
            "I am sure that person heard about Baahubali couple of years back.\n",
            "\n",
            "\n",
            "(1) Parts of Speech (POS) Tagging:\n",
            "Counter({'IN': 3, 'NN': 2, 'PRP': 1, 'VBP': 1, 'JJ': 1, 'VBD': 1, 'NNP': 1, 'NNS': 1, 'RB': 1, '.': 1})\n",
            "\n",
            "\n",
            "(2) Constituency Parsing Tree:\n",
            "(S (NP (NNP Ramya)) (VP (VBZ is) (JJ Sad)))\n",
            "\n",
            "\n",
            "(2) Dependency Parsing Tree:\n",
            "[('I', 'nsubj', 'am'), ('am', 'ROOT', 'am'), ('sure', 'acomp', 'am'), ('that', 'det', 'person'), ('person', 'nsubj', 'heard'), ('heard', 'ccomp', 'sure'), ('about', 'prep', 'heard'), ('Baahubali', 'amod', 'couple'), ('couple', 'pobj', 'about'), ('of', 'prep', 'couple'), ('years', 'pobj', 'of'), ('back', 'advmod', 'heard'), ('.', 'punct', 'am')]\n",
            "\n",
            "\n",
            "(3) Named Entity Recognition (NER):\n",
            "[('Baahubali', 'PERSON')]\n",
            "Counter({'PERSON': 1})\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load spaCy model for NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence for explanation with named entities\n",
        "example_sentence_with_entities = \"I am sure that person heard about Baahubali couple of years back.\"\n",
        "\n",
        "# Function for Parts of Speech (POS) Tagging\n",
        "def ramya_pos_tagging(text):\n",
        "    pos_tags = pos_tag(word_tokenize(text))\n",
        "    pos_counts = nltk.Counter(tag for word, tag in pos_tags)\n",
        "    return pos_counts\n",
        "\n",
        "# Function for Constituency Parsing and Dependency Parsing\n",
        "def ramya_parse_syntax_structure(text):\n",
        "    # Constituency Parsing\n",
        "    constituency_tree_string = \"(S (NP (NNP Ramya)) (VP (VBZ is) (JJ Sad)))\"\n",
        "    ramya_constituency_parsing_tree = Tree.fromstring(constituency_tree_string)\n",
        "\n",
        "    # Dependency Parsing\n",
        "    doc = nlp(text)\n",
        "    ramya_dependency_parsing_tree = [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "    return ramya_constituency_parsing_tree, ramya_dependency_parsing_tree\n",
        "\n",
        "# Function for Named Entity Recognition (NER)\n",
        "def ramya_named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    entity_counts = nltk.Counter(label for _, label in entities)\n",
        "    return entities, entity_counts\n",
        "\n",
        "# Example sentence for explanation\n",
        "print(\"Example Sentence:\")\n",
        "print(example_sentence_with_entities)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "ramya_pos_counts_example = ramya_pos_tagging(example_sentence_with_entities)\n",
        "print(\"(1) Parts of Speech (POS) Tagging:\")\n",
        "print(ramya_pos_counts_example)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "ramya_constituency_tree_example, ramya_dependency_tree_example = ramya_parse_syntax_structure(example_sentence_with_entities)\n",
        "print(\"(2) Constituency Parsing Tree:\")\n",
        "print(ramya_constituency_tree_example)\n",
        "print(\"\\n\")\n",
        "print(\"(2) Dependency Parsing Tree:\")\n",
        "print(ramya_dependency_tree_example)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (3) Named Entity Recognition (NER)\n",
        "ramya_entities_example, ramya_entity_counts_example = ramya_named_entity_recognition(example_sentence_with_entities)\n",
        "print(\"(3) Named Entity Recognition (NER):\")\n",
        "print(ramya_entities_example)\n",
        "print(ramya_entity_counts_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "** *italicized text*Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural language processing uses two syntactic parsing techniques, constituency parsing and dependency parsing, to examine a sentence's grammatical structure. Creating a parse tree that depicts a sentence's syntactic structure is their shared goal.\n",
        "\n",
        "A constituency parsing tree breaks a sentence down into smaller constituents, or phrases, to illustrate the hierarchical structure of a sentence. Words or groups of words can be constituents, and each node in the tree represents a component. Words are represented by leaves, while the phrase as a whole is represented by the root node. Every node in the tree has a label that designates a grammatical category or component of speech, such as preposition, verb, adjective, adverb, or noun.\n",
        "\n",
        "example for constituency prasing tree:\n",
        "Constituency Parsing Tree:\n",
        "(S (NP (NNP Ramya)) (VP (VBZ is) (JJ Sad)))\n",
        "\n",
        "here\n",
        "S represents Sentence\n",
        "NP represents Noun Phrase i.e., Ramya\n",
        "VP represents Verb Phrase i.e., is\n",
        "JJ represnts adjective i.e., Sad\n",
        "\n",
        "Dependency parsing tree shows how a sentence's words relate to one another grammatically. In the phrase, every word is a node inside the tree, with the syntactic relationships between words represented by the edges. All other words in the phrase are related to the primary verb, the root of the tree, according to their syntactic roles. Dependency types, such as subject, object, modifier, etc., are indicated by the labels on the edges.\n",
        "\n",
        "Examples for Dependency parsing tree:\n",
        "\n",
        "Dependency Parsing Tree:\n",
        "[('I', 'nsubj', 'am'), ('am', 'ROOT', 'am'), ('sure', 'acomp', 'am'), ('that', 'det', 'person'), ('person', 'nsubj', 'heard'), ('heard', 'ccomp', 'sure'), ('about', 'prep', 'heard'), ('Baahubali', 'amod', 'couple'), ('couple', 'pobj', 'about'), ('of', 'prep', 'couple'), ('years', 'pobj', 'of'), ('back', 'advmod', 'heard'), ('.', 'punct', 'am')]\n",
        "\n",
        "Explanation:\n",
        "I represents nsubj i.e., nominal subject\n",
        "   am represnts ROOT i.e., verb defines action\n",
        "   sure represents acomp\n",
        "   that represents det i.e., determiner\n",
        "   person represents nsubj i.e., nominal subject\n",
        "   heard represents ccomp\n",
        "   about represents prep\n",
        "   Baahubali represents amod\n",
        "   couple represents pobj\n",
        "   of represents prep i.e., preposition\n",
        "   years represents pobj i.e., prepositional object\n",
        "   back represents advmod\n",
        "   . represents punct i.e., punctuation\n",
        "\n"
      ],
      "metadata": {
        "id": "0I9qB0nwlA7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HG1ePFEqhFMl"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}